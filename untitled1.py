# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19xL-l385tHxefUZb-B8R1A4fvMMiv4iM
"""

#!apt-get install openjdk-8-jdk-headless -qq > /dev/null

"""## Download Spark installer """
!wget -q https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz

!tar xf spark-2.4.6-bin-hadoop2.7.tgz

!pip install -q findspark

import os
#os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.6-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

!wget https://raw.githubusercontent.com/sakksoftware/temp/master/datalake_file.txt

datalake_file = open("datalake_file.txt", "r")
trades = [line.strip().split("| ") for line in datalake_file.readlines()]

trades_events_list = [[tuple(event.split(",")) for event in trade] for trade in trades]

events = [tuple(event.strip().split(",")) for trade in trades for event in trade]

"""## Questions"""
# 1. What are the total number of events?
len(events)

# 2. 
events_df = spark.createDataFrame(events , ["product", "timestamp", "amount"])

#events_df.show()
#events_df.printSchema()
events_df.describe().show()

#trades_df = spark.createDataFrame(trades_events_list, [str(i) for i in range(len(trades_events_list))] )
trades_events_list

trades_df
# list(range(len(trades_events_list)))
#list(range(len(trades_events_list)))
#trades_events_list

#df = spark.read.text("datalake_file.txt")
#data_transposed = zip(trades_events_list)
from pyspark.sql import Row
R = Row("")
#df = spark.createDataFrame([trade for trade in trades_events_list] ).toDF(*list(range(len(trades_events_list))))
#df.show()

trades_length_array = [str(x) for x in list(range(len(trades_events_list)))]

trades_df = spark.createDataFrame(zip(trades_length_array, trades_events_list), ['trade', 'events'])
#rowData = map(lambda x: Row(*x), trades_events_list)
#dfFromData = spark.createDataFrame(rowData, trades_length_array)

trades_df['trade']

#tradesList = [[event.split(",") for event in trade] for trade in trades]
tradesList = [[tuple(event.split(",")) for event in trade] for trade in trades]

#dfTrades = spark.createDataFrame(zip(trades_length_array,trades))
#list(zip(trades_length_array,trades))
#dict(zip(trades_length_array,trades))
trades_events_list

#dfTrades = spark.createDataFrame(dict(zip(trades_length_array,trades)), schema=None)
#trades_events_list
#list(zip(trades_length_array,trades))
from pyspark.sql.types import StructField, StringType, DecimalType, TimestampType, ArrayType, StructType

event_schema = [StructField('product', StringType(), True), 
               StructField('timestamp', TimestampType(), True),
               StructField('price', DecimalType(), True)]

#trade_schema = [StructField('trade', ArrayType(StructField(event_schema)), True)]
trade_schema = [StructField('trade', ArrayType(StructType(event_schema)), True)]

final_schema = StructType(trade_schema)

finalDataFrame = spark.createDataFrame(trades, schema=final_schema)

trades_events_list
#trades

schema

datalake_file = open("datalake_file.txt", "r")

lineList = datalake_file.readlines()

lineList

len(lineList)

!ls

